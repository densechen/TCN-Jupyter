{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-level Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In word-level language modeling tasks, each element of the sequence is a word, where the model is expected to predict the next incoming in the test. We evaluate the temporal convolutional network as a word-level language model on three datasets: PennTreebank (PTB), Wikitext-103. and LAMBADA.\n",
    "\n",
    "Because the evaluation of LAMBADA has different requirement (predicting only the very last word based on a broader context), we put it in another script. See `lambada_language.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff4c8115990>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "BATCH_SIZE = 16\n",
    "DEVICE = \"cuda:0\"\n",
    "DROPOUT = 0.45\n",
    "EMB_DROPOUT = 0.25\n",
    "CLIP = 0.35\n",
    "EPOCHS = 10\n",
    "KSIZE = 3\n",
    "DATA_ROOT = \"/home/densechen/dataset/penn\"\n",
    "EMSIZE = 600\n",
    "LEVELS = 4\n",
    "LR = 4\n",
    "NHID = 600\n",
    "SEED = 1111\n",
    "TIED = True\n",
    "OPTIM = \"SGD\"\n",
    "VALID_SEQ_LEN = 40\n",
    "SEQ_LEN = 80\n",
    "CORPUS = False\n",
    "\n",
    "CHANNEL_SIZES = [NHID] * (LEVELS - 1) + [EMSIZE]\n",
    "EVAL_BATCH_SIZE = 10\n",
    "\n",
    "th.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator\n",
    "\n",
    "The meaning of batch_size in PTB is different from that in MNIST example. In MNIST, \n",
    "batch_size is the # of sample data that is considered in each iteration; in PTB, however,\n",
    "it is the number of segments to speed up computation. \n",
    "\n",
    "The goal of PTB is to train a language model to predict the next word.\n",
    "\n",
    "You should download the dataset from [here](https://github.com/locuslab/TCN/tree/master/TCN/word_cnn/data/penn), and then place it under `DATA_ROOT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producing data...\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def data_generator():\n",
    "    if os.path.exists(os.path.join(DATA_ROOT, \"corpus\")) and not CORPUS:\n",
    "        corpus = pickle.load(open(os.path.join(DATA_ROOT, 'corpus'), 'rb'))\n",
    "    else:\n",
    "        corpus = Corpus(DATA_ROOT)\n",
    "        pickle.dump(corpus, open(os.path.join(DATA_ROOT, 'corpus'), 'wb'))\n",
    "    return corpus\n",
    "\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r') as f:\n",
    "            ids = th.LongTensor(tokens)\n",
    "            token = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "\n",
    "        return ids\n",
    "\n",
    "\n",
    "def batchify(data, batch_size):\n",
    "    \"\"\"The output should have size [L x batch_size], where L could be a long sequence length\"\"\"\n",
    "    # Work out how cleanly we can divide the dataset into batch_size parts (i.e. continuous seqs).\n",
    "    nbatch = len(data) // batch_size\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * batch_size)\n",
    "    # Evenly divide the data across the batch_size batches.\n",
    "    data = data.view(batch_size, -1)\n",
    "    data = data.to(DEVICE)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_batch(source, i, evaluation=False):\n",
    "    seq_len = min(SEQ_LEN, source.size(1) - 1 - i)\n",
    "    data = source[:, i:i+seq_len]\n",
    "    target = source[:, i+1:i+1+seq_len]     # CAUTION: This is un-flattened!\n",
    "    return data, target\n",
    "\n",
    "print(\"Producing data...\")\n",
    "corpus = data_generator()\n",
    "n_words = len(corpus.dictionary)\n",
    "\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(corpus.train, BATCH_SIZE)\n",
    "val_data = batchify(corpus.valid, EVAL_BATCH_SIZE)\n",
    "test_data = batchify(corpus.test, EVAL_BATCH_SIZE)\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Weight tied\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "from core.tcn import TemporalConvNet\n",
    "\n",
    "class TCN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size, num_channels,\n",
    "                 kernel_size=2, dropout=0.3, emb_dropout=0.1, tied_weights=False):\n",
    "        super(TCN, self).__init__()\n",
    "        self.encoder = nn.Embedding(output_size, input_size)\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size, dropout=dropout)\n",
    "\n",
    "        self.decoder = nn.Linear(num_channels[-1], output_size)\n",
    "        if tied_weights:\n",
    "            if num_channels[-1] != input_size:\n",
    "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "            print(\"Weight tied\")\n",
    "        self.drop = nn.Dropout(emb_dropout)\n",
    "        self.emb_dropout = emb_dropout\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"Input ought to have dimension (N, C_in, L_in), where L_in is the seq_len; here the input is (N, L, C)\"\"\"\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        y = self.tcn(emb.transpose(1, 2)).transpose(1, 2)\n",
    "        y = self.decoder(y)\n",
    "        return y.contiguous()\n",
    "\n",
    "print(\"Building model...\")\n",
    "\n",
    "model = TCN(EMSIZE, n_words, CHANNEL_SIZES, \n",
    "            dropout=DROPOUT, emb_dropout=EMB_DROPOUT, kernel_size=KSIZE, tied_weights=TIED)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# May use adaptive softmax to speed up training\n",
    "optimizer = getattr(th.optim, OPTIM)(model.parameters(), lr=LR)\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001d00fd0a3b4064a87f590c62e4533e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1453 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | valid loss 53.68\n",
      "| end of epoch   1 | test loss 52.49\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd2371b0b0e848c890cbb77612172b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1453 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | valid loss 44.76\n",
      "| end of epoch   2 | test loss 43.53\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e8ca3931e14c99ba3793ae2101eb77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1453 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | valid loss 38.78\n",
      "| end of epoch   3 | test loss 37.64\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "275bdf3b102a4a749a8b78e98ade481c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1453 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | valid loss 34.69\n",
      "| end of epoch   4 | test loss 33.50\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c30328dd555498fb3c72938498f5c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1453 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | valid loss 31.75\n",
      "| end of epoch   5 | test loss 30.63\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792c26e0636f4caa9a4388a0f89f47c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1453 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | valid loss 29.38\n",
      "| end of epoch   6 | test loss 28.31\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f77bcde3de44ed81b5a583da167d4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1453 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | valid loss 27.87\n",
      "| end of epoch   7 | test loss 26.88\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e558c5e43a94303bac2983bf57f85b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1453 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | valid loss 26.76\n",
      "| end of epoch   8 | test loss 25.79\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43fb2e2a79e343e598aac1af3a190248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1453 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | valid loss 25.38\n",
      "| end of epoch   9 | test loss 24.48\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da7e72de7a424cfd9aa80b942112989e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1453 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | valid loss 23.91\n",
      "| end of epoch  10 | test loss 23.03\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def evaluate(data_source):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    processed_data_size = 0\n",
    "    with th.no_grad():\n",
    "        for i in range(0, data_source.size(1) - 1, VALID_SEQ_LEN):\n",
    "            if i + SEQ_LEN - VALID_SEQ_LEN >= data_source.size(1) - 1:\n",
    "                continue\n",
    "            data, targets = get_batch(data_source, i, evaluation=True)\n",
    "            output = model(data)\n",
    "\n",
    "            # Discard the effective history, just like in training\n",
    "            eff_history = SEQ_LEN - VALID_SEQ_LEN\n",
    "            final_output = output[:, eff_history:].contiguous().view(-1, n_words)\n",
    "            final_target = targets[:, eff_history:].contiguous().view(-1)\n",
    "\n",
    "            loss = F.cross_entropy(final_output, final_target)\n",
    "\n",
    "            # Note that we don't add TAR loss here\n",
    "            total_loss += (data.size(1) - eff_history) * loss.item()\n",
    "            processed_data_size += data.size(1) - eff_history\n",
    "    return total_loss / processed_data_size\n",
    "\n",
    "\n",
    "def train(ep):\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    process = tqdm(range(0, train_data.size(1) - 1, VALID_SEQ_LEN))\n",
    "    for i in process:\n",
    "        if i + SEQ_LEN - VALID_SEQ_LEN >= train_data.size(1) - 1:\n",
    "            continue\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        # Discard the effective history part\n",
    "        eff_history = SEQ_LEN - VALID_SEQ_LEN\n",
    "        if eff_history < 0:\n",
    "            raise ValueError(\"Valid sequence length must be smaller than sequence length!\")\n",
    "        final_target = targets[:, eff_history:].contiguous().view(-1)\n",
    "        final_output = output[:, eff_history:].contiguous().view(-1, n_words)\n",
    "        loss = F.cross_entropy(final_output, final_target)\n",
    "\n",
    "        loss.backward()\n",
    "        if CLIP > 0:\n",
    "            th.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        optimizer.step()\n",
    "        \n",
    "        process.set_description(f\"Train Epoch: {ep:2d}, loss: {loss.item():.6f}\")\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train(epoch)\n",
    "    val_loss = evaluate(val_data)\n",
    "    test_loss = evaluate(test_data)\n",
    "\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | valid loss {val_loss:5.2f}')\n",
    "    print(f'| end of epoch {epoch:3d} | test loss {test_loss:5.2f}')\n",
    "    print('-' * 89)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
