{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-level Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In character-level language modeling tasks, each sequence is broken into elements by characters. Therefore, in a character-level model, at each time step the model is expected to predict the next coming character. We evaluate the temporal convolutional network as a character-level language model on the PeenTreebank dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4e4c0479d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import observations\n",
    "import unidecode\n",
    "from collections import Counter\n",
    "import time\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DATA_ROOT = \"/home/densechen/dataset\"\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = \"cuda:0\"\n",
    "DROPOUT = 0.1\n",
    "EMB_DROPOUT = 0.1\n",
    "CLIP = 0.15\n",
    "EPOCHS = 10\n",
    "KSIZE = 3\n",
    "LEVELS = 3\n",
    "LR = 4\n",
    "OPTIM = \"SGD\"\n",
    "NHID = 450\n",
    "VALID_SEQ_LEN = 320\n",
    "SEQ_LEN = 400\n",
    "SEED = 1111\n",
    "\n",
    "EMSIZE = 100\n",
    "\n",
    "CHANNEL_SIZES = [NHID] * (LEVELS - 1) + [EMSIZE]\n",
    "\n",
    "th.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Genration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PennTreebank\n",
    "\n",
    "When used as a character-level language corpus, PTB contains 5,059K characters for training, 396K for validation and 446K for testing, with an alphabet size of 50. PennTreebank is a well-studied (but relatively small) language dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producing data...\n",
      "Corpus size: 49\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.char2idx = {}\n",
    "        self.idx2char = []\n",
    "        self.counter = Counter()\n",
    "    \n",
    "    def add_word(self, char):\n",
    "        self.counter[char] += 1\n",
    "    \n",
    "    def prep_dict(self):\n",
    "        for char in self.counter:\n",
    "            if char not in self.char2idx:\n",
    "                self.idx2char.append(char)\n",
    "                self.char2idx[char] = len(self.idx2char) - 1\n",
    "    def __len__(self):\n",
    "        return len(self.idx2char)\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, string):\n",
    "        self.dict = Dictionary()\n",
    "        for c in string:\n",
    "            self.dict.add_word(c)\n",
    "        self.dict.prep_dict()\n",
    "\n",
    "def date_generator():\n",
    "    file, testfile, valfile = observations.ptb(DATA_ROOT)\n",
    "    file_len, valfile_len, testfile_len = len(file), len(valfile), len(testfile)\n",
    "    \n",
    "    corpus = Corpus(file + \" \" + valfile + \" \" + testfile)\n",
    "    \n",
    "    return file, file_len, valfile, valfile_len, testfile, testfile_len, corpus\n",
    "\n",
    "def char_tensor(corpus, string):\n",
    "    tensor = th.zeros(len(string)).long()\n",
    "    for i in range(len(string)):\n",
    "        tensor[i] = corpus.dict.char2idx[string[i]]\n",
    "    return tensor.to(DEVICE)\n",
    "\n",
    "def batchify(data, batch_size):\n",
    "    # the output has size [L x batch size], where L could be a long sequence length.\n",
    "    # work out cleanly we can divide the dataset into batch size parts, i.e. continuous seqs.\n",
    "    nbatch = len(data) // batch_size\n",
    "    # trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * batch_size)\n",
    "    # evently, divide the data across the batch size batches.\n",
    "    data = data.view(batch_size, -1).to(DEVICE)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_batch(source, start_index):\n",
    "    seq_len = min(SEQ_LEN, source.size(1)-1-start_index)\n",
    "    end_index = start_index + seq_len\n",
    "    inp = source[:, start_index:end_index].contiguous()\n",
    "    target = source[:, start_index+1:end_index+1].contiguous()\n",
    "    \n",
    "    return inp, target\n",
    "\n",
    "print(\"Producing data...\")\n",
    "file, file_len, valfile, valfile_len, testfile, testfile_len, corpus = date_generator()\n",
    "\n",
    "n_characters = len(corpus.dict)\n",
    "train_data = batchify(char_tensor(corpus, file), BATCH_SIZE)\n",
    "val_data = batchify(char_tensor(corpus, valfile), 1)\n",
    "test_data = batchify(char_tensor(corpus, testfile), 1)\n",
    "\n",
    "print(f\"Corpus size: {n_characters}\")\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "from core.tcn import TemporalConvNet\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size=2, dropout=0.2, emb_dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Embedding(output_size, input_size)\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.decoder = nn.Linear(input_size, output_size)\n",
    "        self.decoder.weight = self.encoder.weight\n",
    "        self.drop = nn.Dropout(emb_dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # input has dimension (N, L_in), and emb has dimension (N, L_in, C_in).\n",
    "        emb = self.drop(self.encoder(x))\n",
    "        y = self.tcn(emb.transpose(1, 2))\n",
    "        o = self.decoder(y.transpose(1, 2))\n",
    "        return o.contiguous()\n",
    "\n",
    "print(\"Building model...\")\n",
    "\n",
    "model = TCN(EMSIZE, n_characters, CHANNEL_SIZES, KSIZE, DROPOUT, EMB_DROPOUT)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = getattr(th.optim, OPTIM)(model.parameters(), lr=LR)\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "debd97325d2b49c5bb815388b9b6c80a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| End of epoch   1 | valid loss 1.399\n",
      "=========================================================================================\n",
      "| End of epoch   1 | test loss 1.370\n",
      "=========================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e67dea30a4411fb7b185da71fcba8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| End of epoch   2 | valid loss 1.245\n",
      "=========================================================================================\n",
      "| End of epoch   2 | test loss 1.215\n",
      "=========================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d04f2b885ca469e8d7173c71b234a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| End of epoch   3 | valid loss 1.179\n",
      "=========================================================================================\n",
      "| End of epoch   3 | test loss 1.149\n",
      "=========================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ea7f2b8d8f4d2c868b7831ecdbd569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| End of epoch   4 | valid loss 1.140\n",
      "=========================================================================================\n",
      "| End of epoch   4 | test loss 1.110\n",
      "=========================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "509c3c312e234b9196e82725422cdc8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| End of epoch   5 | valid loss 1.115\n",
      "=========================================================================================\n",
      "| End of epoch   5 | test loss 1.084\n",
      "=========================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb8b8a807684f1cba9216813aa75573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| End of epoch   6 | valid loss 1.091\n",
      "=========================================================================================\n",
      "| End of epoch   6 | test loss 1.061\n",
      "=========================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd396e0f85d491dab4434ab6fcd137e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| End of epoch   7 | valid loss 1.074\n",
      "=========================================================================================\n",
      "| End of epoch   7 | test loss 1.044\n",
      "=========================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5104bc604fd4086a0d196175294f32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| End of epoch   8 | valid loss 1.064\n",
      "=========================================================================================\n",
      "| End of epoch   8 | test loss 1.033\n",
      "=========================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115e860a7b9e443c95ba6a94b756d6b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| End of epoch   9 | valid loss 1.053\n",
      "=========================================================================================\n",
      "| End of epoch   9 | test loss 1.023\n",
      "=========================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f938e3de9344898b1bae882a60e9e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| End of epoch  10 | valid loss 1.043\n",
      "=========================================================================================\n",
      "| End of epoch  10 | test loss 1.014\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "def evaluate(source):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    source_len = source.size(1)\n",
    "    count = 0\n",
    "    with th.no_grad():\n",
    "        for batch, i in enumerate(range(0, source_len - 1, VALID_SEQ_LEN)):\n",
    "            if i + SEQ_LEN - VALID_SEQ_LEN >= source_len:\n",
    "                continue\n",
    "            inp, target = get_batch(source, i)\n",
    "            output = model(inp)\n",
    "            eff_history = SEQ_LEN - VALID_SEQ_LEN\n",
    "            final_output = output[:, eff_history:].contiguous().view(-1, n_characters)\n",
    "            final_target = target[:, eff_history:].contiguous().view(-1)\n",
    "            loss = F.cross_entropy(final_output, final_target)\n",
    "\n",
    "            total_loss += loss.data * final_output.size(0)\n",
    "            count += final_output.size(0)\n",
    "\n",
    "    val_loss = total_loss.item() / count * 1.0\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def train(ep):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    source = train_data\n",
    "    source_len = source.size(1)\n",
    "    process = tqdm(range(0, source_len - 1, VALID_SEQ_LEN))\n",
    "    for i in process:\n",
    "        if i + SEQ_LEN - VALID_SEQ_LEN >= source_len:\n",
    "            continue\n",
    "        inp, target = get_batch(source, i)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inp)\n",
    "        eff_history = SEQ_LEN - VALID_SEQ_LEN\n",
    "        final_output = output[:, eff_history:].contiguous().view(-1, n_characters)\n",
    "        final_target = target[:, eff_history:].contiguous().view(-1)\n",
    "        loss = F.cross_entropy(final_output, final_target)\n",
    "        loss.backward()\n",
    "\n",
    "        if CLIP > 0:\n",
    "            th.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        optimizer.step()\n",
    "        \n",
    "        process.set_description(f\"Train Epcoh: {ep}, loss: {loss.item():.4f}\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(epoch)\n",
    "\n",
    "    vloss = evaluate(val_data)\n",
    "    print('-' * 89)\n",
    "    print(f'| End of epoch {epoch:3d} | valid loss {vloss:5.3f}')\n",
    "\n",
    "    test_loss = evaluate(test_data)\n",
    "    print('=' * 89)\n",
    "    print(f'| End of epoch {epoch:3d} | test loss {test_loss:5.3f}')\n",
    "    print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
