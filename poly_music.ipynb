{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polyphnic Music Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "We evaluate temporal convolutional network (TCN) on two popular polyphonic music datasets. The goal here is to predict the next note given some history of the notes played.\n",
    "\n",
    "**NOTE**: \n",
    "- Each sequence can have a different length. In the current implementation, we simply train each sequence separately (i.e. batch size is 1), but one can zero-pad all sequences to the same length and train by batch.\n",
    "\n",
    "- While each data is binary, the fact that there are 88 dimensions (for 88 keys) means there are essentially `2^88` \"classes\". Therefore, instead of directly predicting each key directly, we follow the standard practice so that a sigmoid is added at the end of the network. This ensures that every entry is converted to a value between 0 and 1 to compute the NLL loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fede81a8950>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "DEVICE = \"cuda:0\"\n",
    "DROPOUT = 0.25\n",
    "CLIP = 0.2\n",
    "EPOCHS = 10\n",
    "KSIZE = 5\n",
    "LEVELS = 4\n",
    "LR = 1e-3\n",
    "OPTIM = \"Adam\"\n",
    "NHID = 150\n",
    "\n",
    "DATASET = \"JSB\" # JSB, Muse, Nott, Piano\n",
    "DATA_ROOT = \"/home/densechen/dataset/mdata\"\n",
    "\n",
    "SEED = 1111\n",
    "INPUT_SIZE = 88\n",
    "CHANNEL_SIZES = [NHID] * LEVELS\n",
    "\n",
    "th.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "**JSB Chorales** dataset (Allan & Williams, 2005) is a polyphonic music dataset con-\n",
    "sisting of the entire corpus of 382 four-part harmonized chorales by J. S. Bach. In a polyphonic\n",
    "music dataset, each input is a sequence of elements having 88 dimensions, representing the 88 keys\n",
    "on a piano. Therefore, each element `x_t` is a chord written in as binary vector, in which a “1” indicates\n",
    "a key pressed.\n",
    "\n",
    "**Nottingham** dataset is a collection of 1200 British and American folk tunes. Not-\n",
    "tingham is a much larger dataset than JSB Chorales. Along with JSB Chorales, Nottingham has\n",
    "been used in a number of works that investigated recurrent models’ applicability in polyphonic mu-\n",
    "sic, and the performance for both tasks are measured in terms\n",
    "of negative log-likelihood (NLL) loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producing data...\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "\n",
    "def data_generator():\n",
    "    data = loadmat(\n",
    "        {\"JSB\": os.path.join(DATA_ROOT, \"JSB_Chorales.mat\"),\n",
    "         \"Muse\": os.path.join(DATA_ROOT, \"MuseData.mat\"),\n",
    "         \"Nott\": os.path.join(DATA_ROOT, \"Nottingham.mat\"),\n",
    "         \"Piano\": os.path.join(DATA_ROOT, \"Piano_midi.mat\")}[DATASET])\n",
    "    x_train = data[\"traindata\"][0]\n",
    "    x_valid = data[\"validdata\"][0]\n",
    "    x_test = data[\"testdata\"][0]\n",
    "    \n",
    "    for data in [x_train, x_valid, x_test]:\n",
    "        for i in range(len(data)):\n",
    "            data[i] = th.Tensor(data[i].astype(np.float64))\n",
    "    \n",
    "    return x_train, x_valid, x_test\n",
    "\n",
    "print(\"Producing data...\")\n",
    "x_train, x_valid, x_test = data_generator()\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "from core.tcn import TemporalConvNet\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size, dropout=dropout)\n",
    "        \n",
    "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x needs to have dimension [N, C, L]\n",
    "        output = self.tcn(x.transpose(1, 2)).transpose(1, 2)\n",
    "        output = self.linear(output).double()\n",
    "        return self.sig(output)\n",
    "\n",
    "print(\"Building model...\")\n",
    "\n",
    "model = TCN(INPUT_SIZE, INPUT_SIZE, CHANNEL_SIZES, KSIZE, dropout=DROPOUT)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = getattr(th.optim, OPTIM)(model.parameters(), lr=LR)\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1bdbc2a21d74319811527712bd11072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 10.53107\n",
      "Test loss: 10.63705\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371d787f424e4ecc8d1c9699fe611880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 9.53949\n",
      "Test loss: 9.66112\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48dd78c5d274a73862e76ba748fcc2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 9.26661\n",
      "Test loss: 9.36112\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d33fef044d49b2b5e8049d3d04b84b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 9.04405\n",
      "Test loss: 9.13224\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e06d626a114f0491a35e1ec584e15d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 8.91696\n",
      "Test loss: 9.00111\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee5e08469b040baae9985155b6b570f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 8.80907\n",
      "Test loss: 8.88915\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a841b9f20b4252b9695ac18472c421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 8.73970\n",
      "Test loss: 8.82649\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e1b5583f2464bd98e77a4067dfab374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 8.66689\n",
      "Test loss: 8.73835\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "645b9429d7654eb49259a3dea773c878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 8.61581\n",
      "Test loss: 8.69541\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa806ea632b47f3ac1f93e536a342b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 8.61180\n",
      "Test loss: 8.66965\n"
     ]
    }
   ],
   "source": [
    "def evaluate(x_data, name='Eval'):\n",
    "    model.eval()\n",
    "    eval_idx_list = np.arange(len(x_data), dtype=\"int32\")\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    with th.no_grad():\n",
    "        for idx in eval_idx_list:\n",
    "            data_line = x_data[idx]\n",
    "            x, y = data_line[:-1], data_line[1:]\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            output = model(x.unsqueeze(0)).squeeze(0)\n",
    "            loss = -th.trace(th.matmul(y, th.log(output).float().t()) +\n",
    "                             th.matmul((1-y), th.log(1-output).float().t()))\n",
    "            total_loss += loss.item()\n",
    "            count += output.size(0)\n",
    "    eval_loss = total_loss / count\n",
    "    print(name + \" loss: {:.5f}\".format(eval_loss))\n",
    "\n",
    "def train(ep):\n",
    "    model.train()\n",
    "    train_idx_list = np.arange(len(x_train), dtype=\"int32\")\n",
    "    np.random.shuffle(train_idx_list)\n",
    "    process = tqdm(train_idx_list)\n",
    "    \n",
    "    for idx in process:\n",
    "        data_line = x_train[idx]\n",
    "        x, y = data_line[:-1], data_line[1:]\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x.unsqueeze(0)).squeeze(0)\n",
    "        loss = -th.trace(th.matmul(y, th.log(output).float().t()) +\n",
    "                         th.matmul((1 - y), th.log(1 - output).float().t()))\n",
    "        if CLIP > 0:\n",
    "            th.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        process.set_description(f\"Train Epoch: {ep:2d}, loss: {loss.item():.6f}\")\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    train(ep)\n",
    "    vloss = evaluate(x_valid, name='Validation')\n",
    "    tloss = evaluate(x_test, name='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
