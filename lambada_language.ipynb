{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-level Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "LAMBADA is a collection of narrative passages sharing the characteristics such that human subjects are able to guess accurately given sufficient context, but not so if they only see the last sentence containing the target word. On average, the context contains 4.6 sentence, and the testing performance is evaluated by having the model the last element of the target sentence (i.e. the very last word).\n",
    "\n",
    "Most of the existing computational models fail on this task (without the help of external memory unit, such as neural cache). See the original LAMBADA [paper](https://arxiv.org/pdf/1606.06031.pdf) for more results on applying RNNs on LAMBADA.\n",
    "\n",
    "**Examples**:\n",
    "\n",
    "```\n",
    "Context: \"Yes, I thought I was going to lose the baby.\" \"I was scared too,\" he stated, sincerity flooding his eyes. \"You were?\" \"Yes, of course. Why do you even ask?\" \"This baby wasn't exactly planned for.\"\n",
    "\n",
    "Target sentence: \"Do you honestly think that I would want you to have a ____\"\n",
    "\n",
    "Target word: miscarriage\n",
    "```\n",
    "\n",
    "**NOTE**: \n",
    "\n",
    "- Just like in a recurrent network implementation, where it is common to repackage hidden units when a new sequence begins, we pass into TCN a sequence `T` consisting of two parts: \n",
    "    1) effective history `L1`.\n",
    "    2) valid sequence `L2`.\n",
    "```\n",
    "Sequence [------T------>] = [--L1--> ------L2------>]\n",
    "```\n",
    "\n",
    "- In the forward pass, the whole sequence is passed into TCN, but only the `L2` portion is used for training. This ensures that the training data are also provided with sufficient history. The size of `T` and `L2` can be adjusted via flag `seq_len` and `validseqlen`.\n",
    "\n",
    "- The choice of data to load can be specified via the `data` flag, followed by the path to the directory containing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff5e80a1990>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "BATCH_SIZE = 20\n",
    "DEVICE = \"cuda:1\"\n",
    "DROPOUT = 0.1\n",
    "EMB_DROPOUT = 0.1\n",
    "CLIP = 0.4\n",
    "EPOCHS = 10\n",
    "KSIZE = 4\n",
    "DATA_ROOT = \"/home/densechen/dataset\"\n",
    "EMSIZE = 500\n",
    "LEVELS = 5\n",
    "LR = 4\n",
    "NHID = 500\n",
    "SEED = 1111\n",
    "TIED = True\n",
    "OPTIM = \"SGD\"\n",
    "VALID_SEQ_LEN = 50\n",
    "SEQ_LEN = 100\n",
    "CORPUS = False\n",
    "\n",
    "CHANNEL_SIZES = [NHID] * (LEVELS - 1) + [EMSIZE]\n",
    "\n",
    "th.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "The meaning of batch size in PTB is different from that in MNIST example. In MNIST, batch size is the # of sample data that is considered in each iteration; in PTB, however, it is the number of segments to speed up computation.\n",
    "\n",
    "The goal of PTB is to train a language model to predict the next word.\n",
    "\n",
    "**NOTE**: You will need to download the lambada dataset from [here](http://clic.cimec.unitn.it/lambada/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producing data...\n",
      "torch.Size([20, 737368])\n",
      "Total # of words: 112747\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import re\n",
    "\n",
    "def data_generator():\n",
    "    if os.path.exists(os.path.join(DATA_ROOT, \"corpus\")) and not CORPUS:\n",
    "        corpus = pickle.load(open(os.path.join(DATA_ROOT, \"corpus\"), \"rb\"))\n",
    "    else:\n",
    "        print(\"Creating Corpus...\")\n",
    "        corpus = Corpus(os.path.join(DATA_ROOT,  \"lambada-vocab-2.txt\"), DATA_ROOT)\n",
    "        pickle.dump(corpus, open(os.path.join(DATA_ROOT, \"corpus\"), \"wb\"))\n",
    "    \n",
    "    train_data = batchify(corpus.train, BATCH_SIZE)\n",
    "    val_data = [[0] * (SEQ_LEN - len(line)) + line for line in corpus.valid]\n",
    "    test_data = [[0] * (SEQ_LEN - len(line)) + line for line in corpus.test]\n",
    "    return train_data, val_data, test_data, corpus\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, dict_path, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.prep_dict(dict_path)\n",
    "        self.train = th.LongTensor(self.tokenize(os.path.join(path, \"train-novels\")))\n",
    "        self.valid = self.tokenize(os.path.join(path, \"lambada_development_plain_text.txt\"), eval=True)\n",
    "        self.test = self.tokenize(os.path.join(path, \"lambada_test_plain_text.txt\"), eval=True)\n",
    "    \n",
    "    def prep_dict(self, dict_path):\n",
    "        assert os.path.exists(dict_path)\n",
    "        \n",
    "        # Add words to the dictionary\n",
    "        with open(dict_path, \"r\") as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                word = line.strip()\n",
    "                tokens += 1\n",
    "                self.dictionary.add_word(word)\n",
    "        \n",
    "        if \"<eos>\" not in self.dictionary.word2idx:\n",
    "            self.dictionary.add_word(\"<eos>\")\n",
    "            tokens += 1\n",
    "        \n",
    "        print(f\"The dictionary captured a covalbulary of size {tokens}\")\n",
    "    \n",
    "    def tokenize(self, path, eval=False):\n",
    "        assert os.path.exists(path)\n",
    "        \n",
    "        ids = []\n",
    "        token = 0\n",
    "        misses = 0\n",
    "        if not path.endswith(\".txt\"):\n",
    "            for subdir in os.listdir(path):\n",
    "                for filename in os.listdir(path + \"/\" + subdir):\n",
    "                    if filename.endswith(\".txt\"):\n",
    "                        full_path = os.path.join(path, subdir, filename)\n",
    "                        # Tokenize file content\n",
    "                        delta_ids, delta_token, delta_miss = self._tokenize_file(full_path, eval=eval)\n",
    "                        ids += delta_ids\n",
    "                        token += delta_token\n",
    "                        misses += delta_miss\n",
    "                    \n",
    "        else:\n",
    "            ids, token, misses = self._tokenize_file(path, eval=eval)\n",
    "\n",
    "        print(token, misses)\n",
    "        return ids\n",
    "\n",
    "    def _tokenize_file(self, path, eval=False):\n",
    "        with open(path, \"r\") as f:\n",
    "            token = 0\n",
    "            ids = []\n",
    "            misses = 0\n",
    "            for line in f:\n",
    "                line_ids = []\n",
    "                words = line.strip().split() + [\"<eos>\"]\n",
    "                if eval:\n",
    "                    words = words[:-1]\n",
    "                for word in words:\n",
    "                    # these words are in the text but not vocabulary\n",
    "                    if word == \"n't\":\n",
    "                        word = \"not\"\n",
    "                    elif word == \"'s\":\n",
    "                        word = \"is\"\n",
    "                    elif word == \"'re\":\n",
    "                        word = \"are\"\n",
    "                    elif word == \"'ve\":\n",
    "                        word = \"have\"\n",
    "                    elif word == \"wo\":\n",
    "                        word = \"will\"\n",
    "                    if word not in self.dictionary.word2idx:\n",
    "                        word = re.sub(r'[^\\w\\s]', '', word)\n",
    "                    if word not in self.dictionary.word2idx:\n",
    "                        misses += 1\n",
    "                        continue\n",
    "                    line_ids.append(self.dictionary.word2idx[word])\n",
    "                    token += 1\n",
    "                if eval:\n",
    "                    ids.append(line_ids)\n",
    "                else:\n",
    "                    ids += line_ids\n",
    "        return ids, token, misses\n",
    "\n",
    "def batchify(data, batch_size):\n",
    "    \"\"\"the output should have size [L x batch size], where L could be a long sequence length.\n",
    "    \"\"\"\n",
    "    # work out how cleanly we can divide the dataset into batch size parts\n",
    "    # i.e. continuous seqs.\n",
    "    nbatch = len(data) // batch_size\n",
    "    # trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * batch_size)\n",
    "    # evently, divide the data across the batch size batches.\n",
    "    data = data.view(batch_size, -1)\n",
    "    print(data.size())\n",
    "    data = data.to(DEVICE)\n",
    "    return data\n",
    "\n",
    "def get_batch(source, i, seq_len=None, evaluation=False):\n",
    "    seq_len = min(SEQ_LEN, source.size(1) - 1 - i)\n",
    "    data = source[:, i:i+seq_len]\n",
    "    target = source[:, i+1:i+1+seq_len]\n",
    "    \n",
    "    return data, target\n",
    "\n",
    "print(\"Producing data...\")\n",
    "train_data, val_data, test_data, corpus = data_generator()\n",
    "\n",
    "n_words = len(corpus.dictionary)\n",
    "print(f\"Total # of words: {n_words}\")\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "weight tied\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "from core.tcn import TemporalConvNet\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, \n",
    "                 kernel_size=2, dropout=0.3, emb_dropout=0.1, tied_weights=False):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Embedding(output_size, input_size)\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size, dropout=dropout)\n",
    "        \n",
    "        self.decoder = nn.Linear(num_channels[-1], output_size)\n",
    "        if tied_weights:\n",
    "            if num_channels[-1] != input_size:\n",
    "                raise ValueError(\"When using the tied flag, nhid must be equal to emsize\")\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "            print(\"weight tied\")\n",
    "        \n",
    "        self.drop = nn.Dropout(emb_dropout)\n",
    "        self.emb_dropout = emb_dropout\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"Input ought to have dimension (N, C_in, L_in), where L_in is the \n",
    "        seq_len, here the input is (N, L, C).\n",
    "        \"\"\"\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        y = self.tcn(emb.transpose(1, 2)).transpose(1, 2)\n",
    "        y = self.decoder(y)\n",
    "        return y.contiguous()\n",
    "    \n",
    "\n",
    "\n",
    "print(\"Building model...\")\n",
    "model = TCN(EMSIZE, n_words, CHANNEL_SIZES, dropout=DROPOUT,\n",
    "            emb_dropout=EMB_DROPOUT, kernel_size=KSIZE, tied_weights=TIED)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = getattr(th.optim, OPTIM)(model.parameters(), lr=LR)\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a679d77b8d2a496694479c4525cf26e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14748 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate(data_source):\n",
    "    model.eval()\n",
    "    processed_data_size = 0\n",
    "    total_loss = 0\n",
    "    with th.no_grad():\n",
    "        for i in range(len(data_source)):\n",
    "            data, targets = th.LongTensor(data_source[i]).view(1, -1), th.LongTensor([data_source[i][-1]]).view(1, -1)\n",
    "            data, targets = data.to(DEVICE), targets.to(DEVICE)\n",
    "            output = model(data)\n",
    "            final_output = output[:, -1].contiguous().view(-1, n_words)\n",
    "            final_target = targets[:, -1].contiguous().view(-1)\n",
    "            loss = F.cross_entropy(final_output, final_target)\n",
    "            processed_data_size += 1\n",
    "            total_loss += loss\n",
    "    return total_loss.item() / processed_data_size\n",
    "\n",
    "def train(ep):\n",
    "    model.train()\n",
    "    process = tqdm(range(0, train_data.size(1) - 1, VALID_SEQ_LEN))\n",
    "    for i in process:\n",
    "        if i + SEQ_LEN - VALID_SEQ_LEN >= train_data.size(1) - 1:\n",
    "            continue\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        eff_history = SEQ_LEN - VALID_SEQ_LEN\n",
    "        if eff_history < 0:\n",
    "            raise ValueError(\"Valid sequence length must be smaller than sequence length!\")\n",
    "        final_target = targets[:, eff_history:].contiguous().view(-1)\n",
    "        final_output = output[:, eff_history:].contiguous().view(-1, n_words)\n",
    "        loss = F.cross_entropy(final_output, final_target)\n",
    "        loss.backward()\n",
    "        if CLIP > 0:\n",
    "            th.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        optimizer.step()\n",
    "\n",
    "        process.set_description(f\"Train Epoch: {ep:2d}, loss: {loss.item():.4f}\")\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train(epoch)\n",
    "    val_loss = evaluate(val_data)\n",
    "    test_loss = evaluate(test_data)\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | valid loss {val_loss:5.2f}')\n",
    "    print(f'| end of epoch {epoch:3d} | test loss {test_loss:5.2f}')\n",
    "    print('-' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
